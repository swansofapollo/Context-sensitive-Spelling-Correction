{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corrected word: bade\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams as nltk_ngrams\n",
        "\n",
        "# Function to load N-gram data from a file\n",
        "def load_ngrams(filename):\n",
        "    ngrams = defaultdict(Counter)\n",
        "    with open(filename, 'r', encoding='ISO-8859-1') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) == 2:\n",
        "                ngram, count = parts\n",
        "                words = ngram.split(' ')\n",
        "                if len(words) == 2:\n",
        "                    prefix, word = words\n",
        "                    ngrams[prefix][word] += int(count)\n",
        "    return ngrams\n",
        "\n",
        "# Function to preprocess text by removing headers and footers specific to Project Gutenberg files\n",
        "def preprocess_text(filename):\n",
        "    lines = []\n",
        "    start_reading = False\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            if line.startswith('*** START OF THIS PROJECT GUTENBERG EBOOK'):\n",
        "                start_reading = True\n",
        "                continue\n",
        "            elif line.startswith('*** END OF THIS PROJECT GUTENBERG EBOOK'):\n",
        "                break\n",
        "            if start_reading:\n",
        "                lines.append(line.strip())\n",
        "    return ' '.join(lines)\n",
        "\n",
        "# Function to build a bigram model from text\n",
        "def build_bigram_model(text):\n",
        "    model = defaultdict(Counter)\n",
        "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
        "    for w1, w2 in nltk_ngrams(tokens, 2, pad_right=True, pad_left=True, left_pad_symbol=None, right_pad_symbol=None):\n",
        "        model[w1][w2] += 1\n",
        "    return model\n",
        "\n",
        "# Function to generate all possible edits that are one edit away from the input word\n",
        "def edits1(word):\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [L + R[1:] for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "    inserts = [L + c + R for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "# Function to filter and return only the known words from a set of candidate words\n",
        "def known(words, ngrams):\n",
        "    return set(w for w in words if w in ngrams)\n",
        "\n",
        "# Function to correct a given word based on its context (previous word) and the ngram model\n",
        "def correct_word(word, previous_word, ngrams):\n",
        "    candidates = known([word], ngrams) or known(edits1(word), ngrams) or [word]\n",
        "    corrected_word = max(candidates, key=lambda candidate: word_probability(candidate, previous_word, ngrams))\n",
        "    return corrected_word\n",
        "\n",
        "# Function to calculate the probability of a word given the previous word using the bigram model\n",
        "def word_probability(word, previous_word, ngrams):\n",
        "    if previous_word in ngrams and word in ngrams[previous_word]:\n",
        "        return ngrams[previous_word][word] / sum(ngrams[previous_word].values())\n",
        "    else:\n",
        "        return 1 / sum(ngrams[previous_word].values()) if previous_word in ngrams else 0\n",
        "\n",
        "# Example usage\n",
        "bigrams = load_ngrams('bigrams.txt')\n",
        "fivegrams = load_ngrams('fivegrams.txt')  # Example loading, though not used in the given code\n",
        "\n",
        "# Process and load the 'big.txt' for additional training data\n",
        "text = preprocess_text('big.txt')\n",
        "bigram_model_from_text = build_bigram_model(text)\n",
        "\n",
        "# Combine the models (optional, depending on your use case)\n",
        "# This simplistic approach adds counts; for more nuanced merging, consider adjusting probabilities\n",
        "for prev_word, following_words in bigram_model_from_text.items():\n",
        "    for word, count in following_words.items():\n",
        "        bigrams[prev_word][word] += count\n",
        "\n",
        "# Correct an example word\n",
        "previous_word = 'a'\n",
        "incorrect_word = 'bae'\n",
        "corrected_word = correct_word(incorrect_word, previous_word, bigrams)\n",
        "print(f'Corrected word: {corrected_word}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "Use of N-gram Models: The core of the spelling correction system is based on bigram models. This choice allows the system to consider the context of the word being corrected by looking at its immediate predecessor. Bigram models are simple yet effective for capturing local context, which is often sufficient for spelling correction.\n",
        "\n",
        "DefaultDict and Counter for Model Storage: The use of defaultdict(Counter) for storing n-gram frequencies simplifies the management of counts and probabilities. This data structure choice allows for efficient lookups and updates, which are crucial for performance, especially when working with large text corpora.\n",
        "\n",
        "Edit Distance Operations in edits1 Function: The implementation of the edits1 function, which generates all possible edits that are one edit away from the input word, uses four types of operations: deletions, transpositions, replacements, and insertions. These operations cover a wide range of common spelling mistakes, making the corrector versatile in handling different types of errors.\n",
        "\n",
        "Word Probability Calculation: The word_probability function computes the likelihood of a word given its previous word using the frequencies from the bigram model. This probabilistic approach helps in selecting the most likely correction among several candidates by leveraging statistical information from the text data.\n",
        "\n",
        "Correction Based on Context and Edit Distance: The correct_word function combines contextual information and edit distance operations to find the best correction. It first tries to find the word in the model (zero edits), then looks for one-edit corrections known to the model, and defaults to the original word if no known corrections are found. This prioritization ensures that more probable corrections are favored, and the original word is preserved when no confident correction can be made.\n",
        "\n",
        "Incorporating Additional Training Data: The code snippet shows an approach to merging bigram models from different sources (e.g., pre-loaded bigrams and those built from a new text corpus). This strategy enhances the corrector's robustness by broadening its vocabulary and contextual understanding, thereby improving its accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speling\n",
            "korrectud\n",
            "bycycle\n",
            "inconvient\n",
            "arranged\n",
            "example\n",
            "writing\n",
            "beautiful\n",
            "response\n",
            "relieve\n",
            "Accuracy of Context-Sensitive Corrector: 0.50\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Context-sensitive spelling corrector wrapper\n",
        "def context_sensitive_wrapper(word, previous_word):\n",
        "    return correct_word(word, previous_word, bigrams)  \n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_spelling_corrector(corrector, test_set):\n",
        "    correct_count = 0\n",
        "    for previous_word, incorrect, correct in test_set:\n",
        "\n",
        "        predicted = corrector(incorrect, previous_word)\n",
        "        print(predicted )\n",
        "        if predicted == correct:\n",
        "            correct_count += 1\n",
        "    accuracy = correct_count / len(test_set)\n",
        "    return accuracy\n",
        "\n",
        "# Test set\n",
        "test_set_with_context = [\n",
        "    ('a', 'speling', 'spelling'),  # deletion\n",
        "    ('the', 'korrectud', 'corrected'),  # substitution\n",
        "    ('my', 'bycycle', 'bicycle'),  # insertion\n",
        "    ('very', 'inconvient', 'inconvenient'),  # transposition\n",
        "    ('they', 'arrainged', 'arranged'),  # mixed\n",
        "    ('an', 'exampel', 'example'),  # substitution\n",
        "    ('to', 'writting', 'writing'),  # double letter\n",
        "    ('a', 'beutiful', 'beautiful'),  # substitution\n",
        "    ('his', 'responce', 'response'),  # common mistake\n",
        "    ('we', 'recieve', 'receive')  # ie/ei confusion\n",
        "]\n",
        "\n",
        "# Placeholder for context-sensitive accuracy since the actual function is not implemented\n",
        "accuracy_context_sensitive = evaluate_spelling_corrector(context_sensitive_wrapper, test_set_with_context)\n",
        "\n",
        "print(f\"Accuracy of Context-Sensitive Corrector: {accuracy_context_sensitive:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
